---
layout: default 
---
<header class="bloghead">
    <h1 class="bloghead-title">
    <a href="{{ site.url }}/">{{ site.name }}<span>&#39;s homepage</span></a>
  </h1>
    <nav class="bloghead-nav">
        {% for nav in site.nav %}
        <a href="{{ nav.href }}">{{ nav.name }}<span> &nbsp;/&nbsp; </span></a> {% endfor %}
    </nav>
</header>
<div class="content">
    <a name="bio"></a>
    <div class="post-list">
        <h2 class="post-title">
        Bio
    	</h2> 
        <p> I am a founding member at Amazon Generative Foundational AI, building LLMs from scratch for Amazon (i.e. <a href="https://www.aboutamazon.com/news/retail/amazon-rufus" style="color:#4133ff;">Rufus</a>). Priviously, I worked at Amazon <a href="https://amazonsearchqu.github.io/" style="color:#4133ff;">QU</a> team. I dropped out of <a href="https://www.cs.washington.edu/research/nlp/people" style="color:#4133ff;">UW CS PhD program (NLP track)</a> in 2021 due to some personal reasons. I received M.S. in Computer Science at <a href="https://www.gatech.edu/" style="color:#4133ff;">Georgia Tech</a> in 2021, where I worked with Prof. <a href="https://cs.stanford.edu/~diyiy/" style="color:#4133ff;">Diyi Yang</a> at SALT lab, currently at <a href="https://nlp.stanford.edu/people/" style="color:#4133ff;">Stanford University</a>. Before that,  I received B.S. in Biological Science and Computer Science at <a href="http://english.pku.edu.cn/" style="color:#4133ff;">Peking University</a> in 2019. I also did research at <a href="https://research.google/" style="color:#4133ff;">Google</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" style="color:#4133ff;">Microsoft</a> as an intern. </p>
        
        <p> Nowadays, I'm mainly working on LLMs <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog: Reproduction and Usage of GPT3/ChatGPT]</a>, including 1) pretraining (data, infrastructure, scaling laws) 2) post-training (instruction tuning, human and AI preference learning) 3) evaluation 4) language agents (tool using, planning and reasoning, long-context handling) 5) alignment and AI safety <a href="https://jingfengyang.github.io/safety" style="color:#4133ff;">[Blog: AI Safety: Why, What, and How]</a>.</p>

        <p> The major question I'm thinking about is where is the boundary between LLM capability and alignment <a href="https://jingfengyang.github.io/alignment" style="color:#4133ff;">[Blog: Capability and Alignment]</a>.  </p>

        <p> Before the era of LLMs, I studied Natural Language Processing <a href="https://zhuanlan.zhihu.com/p/539706909" style="color:#4133ff;">[Blog: NLP Trends from the perspective of LLM, i.e. summary of ACL 2022 (in Chinese)]</a>, Machine Learning, and large-scale pretraining of unified multimodal, multitask, multilingual Transformer models [<a href="https://github.com/JingfengYang/Multi-modal-Deep-Learning" style="color:#4133ff;">Repo: Paper List of Multi-modal Deep Learning</a>].</p>

        <p> My previous work on semantic parsing, compositional generalization, text generation, code generation, multilinguality and multi-modality makes me particularly interested in langauge agents, reasoning and grounding. </p>

        <p> If you're looking for collaboration, or seeking any advice that I might give, feel free to schedule a short (non-commercial) <a href="https://calendly.com/jingfengyang/30min" style="color:#4133ff;">research chat</a>. </p>

        <p> Our Amazon Team also has research intern / fulltime oppenings in Generative AI. If you are interested, shoot me an email. </p>
 

    </div>
    <a name="news"></a>
    <div class="post-list">
        <h2 class="post-title">
        News
        </h2> 
        <p><b>Aug 11th, 2024:</b> New paper on LLM Inductive v.s. Deductive Reasoning reported by Forbes. <a href="https://arxiv.org/abs/2408.00114v1" style="color:#4133ff;">[Paper]</a> <a href="https://www.forbes.com/sites/lanceeliot/2024/08/11/on-whether-generative-ai-and-large-language-models-are-better-at-inductive-reasoning-or-deductive-reasoning-and-what-this-foretells-about-the-future-of-ai/" style="color:#4133ff;">[Forbes Media Coverage]</a> </p> 
        <p><b>March 16th, 2024:</b> We are hosting KDD Cup 2024: <a href="https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms" style="color:#4133ff;">Multi-Task Online Shopping Challenge for LLMs</a>. Welcome to participate! Although I personally believe LLMs are generalists and we should not optimize them towards any tasks, an AI Shopping Assistant is still useful (e.g. as a Language Agent)! </p> 
        <p><b>Feb 14th, 2024:</b> New Blog Post on "Capability or Alignment? Respect the LLM Base Model's Capability During Alignment". <a href="https://jingfengyang.github.io/alignment" style="color:#4133ff;">[Blog]</a> <a href="https://x.com/JingfengY/status/1759310453782753416?s=20" style="color:#4133ff;">[Tweets]</a> </p> 
        <p><b>May 9th, 2023:</b> New Blog Post on "AI Safety: Why, What, and How". <a href="https://jingfengyang.github.io/safety" style="color:#4133ff;">[Blog]</a> <a href="https://twitter.com/JingfengY/status/1656143046185201665" style="color:#4133ff;">[Tweets]</a> <a href="https://x.com/ylecun/status/1651762787373428736" style="color:#4133ff;">[Yann LeCun's Post]</a> </p> 
        <p><b>April 29th, 2023:</b> New survey "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond" is on arXiv. <a href="https://arxiv.org/abs/2304.13712" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/Mooler0410/LLMsPracticalGuide" style="color:#4133ff;">[Repo]</a></p> 
        <p><b>Feb 13th, 2023:</b> New Blog Post on Reproduction and Usage of GPT3/ChatGPT. <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog]</a> <a href="https://zhuanlan.zhihu.com/p/607744955" style="color:#4133ff;">[Blog (in Chinese)]</a> <a href="https://twitter.com/JingfengY/status/1625003999387881472" style="color:#4133ff;">[Tweets]</a> </p> 
        <p><b>Apr 7th, 2022:</b> Our paper "SUBS: Subtree Substitution for Compositional Semantic Parsing." is accepted at NAACL 2022. <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog: Compositional Generalization (in Chinese)]</a> </p>
        <p><b>Apr 7th, 2022:</b> Our paper "Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models." is accepted at NAACL 2022 Findings. </p>
        <p><b>Feb 23th, 2022:</b> Our paper "TableFormer: Robust Transformer Modeling for Table-Text Encoding." is accepted at ACL 2022 (<font color="red">Oral</font>). <a href="https://mp.weixin.qq.com/s/ikuHpGt3nYYkitUYlesdfA" style="color:#4133ff;">[Blog (in Chinese)]</a> <a href="https://github.com/google-research/tapas/blob/master/TABLEFORMER.md" style="color:red">[Code Released]</a>
        <p><b>Jan 17th, 2022:</b> I returned to Amazon Search as an Applied Scientist. </p>
        <p><b>Sep 27th, 2021:</b> I started my Applied Scientist internship at Amazon. </p>
        <p><b>Aug 26th, 2021:</b> Our paper "Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing." is accepted at EMNLP 2021. </p>
        <p><b>Jul 12th, 2021:</b> I started my Research Internship at Google. </p>
        <p><b>Sep 15th, 2020:</b> Our paper "Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection." is accepted at EMNLP 2020. </p>
    </div>
    <a name="education"></a>
    <div class="post-list">
        <h2 class="post-title">
        Education
    	</h2> 
        <p>Master of Science: Computer Science, Georgia Tech, Atlanta, America.  Aug 2019 - May 2021.</p>
        <p>Bachelor of Science: Computer Science and Biological Science, Peking University, Beijing, China.  Sep 2015 - July 2019.</p>
    </div>
    <a name="blog"></a>
    <div class="post-list">
        <h2 class="post-title">
        Blog
        </h2> 
        <ul>
            <li><b>Jingfeng Yang</b>. 2024. <b>Capability or Alignment? Respect the LLM Base Model's Capability During Alignment.</b> Blog Post, Feb 2024. <a href="https://jingfengyang.github.io/alignment" style="color:#4133ff;">[Blog]</a> <a href="https://x.com/JingfengY/status/1759310453782753416?s=20" style="color:#4133ff;">[Tweets]</a> </li> 
            <li><b>Jingfeng Yang</b>. 2023. <b>AI Safety: Why, What, and How.</b> Blog Post, May 2023. <a href="https://jingfengyang.github.io/safety" style="color:#4133ff;">[Blog]</a> <a href="https://zhuanlan.zhihu.com/p/630035644" style="color:#4133ff;">[Blog (in Chinese)]</a>  <a href="https://twitter.com/JingfengY/status/1656143046185201665" style="color:#4133ff;">[Tweets]</a> </li> 
            <li><b>Jingfeng Yang</b>. 2023. <b>Why Did All of the Public Reproduction of GPT-3 Fail? In Which Tasks Should We Use GPT-3.5/ChatGPT?</b> Blog Post, Feb 2023. <a href="https://jingfengyang.github.io/gpt" style="color:#4133ff;">[Blog]</a> <a href="https://zhuanlan.zhihu.com/p/607744955" style="color:#4133ff;">[Blog (in Chinese)]</a> <a href="https://twitter.com/JingfengY/status/1625003999387881472" style="color:#4133ff;">[Tweets]</a>  </li> 
            <li><b>Jingfeng Yang</b>. 2022. <b>NLP Trends from the Perspective of LLM, i.e. Summary of ACL 2022. </b> Blog Post, July 2022. <a href="https://zhuanlan.zhihu.com/p/539706909" style="color:#4133ff;">[Blog (in Chinese)]</a> </li> 
            <li><b>Jingfeng Yang</b>. 2022. <b>The Dark Matter towards AGI: Compositional Generalization. </b> Blog Post, April 2022. <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog (in Chinese)]</a></li> 
            <li><b>Jingfeng Yang</b>. 2022. <b>TableFormer: Robust Transformer Modeling for Table-Text Encoding. </b> Blog Post, March 2022. <a href="https://mp.weixin.qq.com/s/ikuHpGt3nYYkitUYlesdfA" style="color:#4133ff;">[Blog (in Chinese)]</a></li>
        </ul>
    </div>
    <a name="publications"></a>
    <div class="post-list">
        <h2 class="post-title">
        Publications 
        </h2> 
        <h3 class="post-title">
            Selected Work
        </h3> 
        <ul>
            <li><b>Jingfeng Yang&#42;</b>, Hongye Jin&#42;, Ruixiang Tang&#42;, Xiaotian Han&#42;, Qizhang Feng&#42;, Haoming Jiang, Bing Yin, Xia Hu. 2023. <b>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.</b> TKDD 2024. <a href="https://arxiv.org/abs/2304.13712" style="color:#4133ff;">[Paper]</a> <a href="https://x.com/ylecun/status/1651762787373428736" style="color:#4133ff;">[Yann LeCun's Post]</a>  <a href="https://github.com/Mooler0410/LLMsPracticalGuide" style="color:#4133ff;">[Repo]</a> <a class="github-button" href="https://github.com/Mooler0410/LLMsPracticalGuide" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-star" data-show-count="true" aria-label="Star buttons/github-buttons on GitHub">Star</a>. Figure of LLM Evolutionary Tree (Credits to Hongye Jin): </li>
            <img src="resources/figures/tree.jpg" alt="LLM Evolutionary Tree" width="400" height="300" class="center"> <br/><br/> 
            <li><b>Jingfeng Yang</b>, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, Diyi Yang. 2022. <b>SEQZERO: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models.</b> In NAACL' 2022 (Findings). <a href="https://arxiv.org/pdf/2205.07381.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/amzn/SeqZero" style="color:#4133ff;">[Code]</a> <a href="resources/slides/SeqZero_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog (in Chinese)]</a> <br/><br/> -- 1) <a href="https://openreview.net/forum?id=ZB58jLvryqw"  style="color:#4133ff;">The first one (Jan 16th, 2022 on OpenReview)</a> to propose Problem Decomposition and Sequential Prompting LLMs for better compositional generalization, even before Chain-of-Thought Prompting and Least-to-Most Prompting, although CoT and Least-to-Most prompting could achieve better performance with larger LMs in a wider range of tasks :). 2) The first one to try model merging (model weight ensemble) with LLMs for generation tasks (refer to notes in related work). 3) Proposed to combine constrained rescaling (constrained decoding) with model ensemble to make ensemble work for LLM generation. </li> <br/><br/> 
            <li><b>Jingfeng Yang</b>, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, Shachi Paul. 2022. <b>TableFormer: Robust Transformer Modeling for Table-Text Encoding.</b> In ACL' 2022 (<font color="red">Oral</font>). <a href="https://arxiv.org/pdf/2203.00274.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/google-research/tapas/blob/master/TABLEFORMER.md" style="color:#4133ff;">[Code]</a> <a class="github-button" href="https://github.com/google-research/tapas" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-star" data-show-count="true" aria-label="Star buttons/github-buttons on GitHub">Star</a> <a href="resources/slides/TableFormer_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/ikuHpGt3nYYkitUYlesdfA" style="color:#4133ff;">[Blog (in Chinese)]</a> <br/><br/> -- The first one to identify the vulnerability to row/column perturbations for prior table-text encoding models. Designed and pretrained a TableFormer model, where learnable attention biases could help achieve strict robustness.</li>  
        </ul>
        <h3 class="post-title">
            Other Publications and Preprints
        </h3> 
        <ul>
            <li>Kewei Cheng, <b>Jingfeng Yang</b>, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, Bing Yin, Yizhou Sun. 2024. <b>Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs.</b> ACL 2024 NLRSE Workshop. <a href="https://arxiv.org/abs/2408.00114v1" style="color:#4133ff;">[Paper]</a> <a href="https://www.forbes.com/sites/lanceeliot/2024/08/11/on-whether-generative-ai-and-large-language-models-are-better-at-inductive-reasoning-or-deductive-reasoning-and-what-this-foretells-about-the-future-of-ai/" style="color:#4133ff;">[Forbes Media Coverage]</a>  </li>
            <li>Hongye Jin, Xiaotian Han, <b>Jingfeng Yang</b>, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu. 2024. <b>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning.</b> ICML 2024. <a href="https://arxiv.org/pdf/2401.01325.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://x.com/ylecun/status/1748526916104020125" style="color:#4133ff;">[Yann LeCun's Post]</a> <a href="https://github.com/datamllab/LongLM" style="color:#4133ff;">[Code]</a> <a class="github-button" href="https://github.com/datamllab/LongLM" data-color-scheme="no-preference: light; light: light; dark: dark;" data-icon="octicon-star" data-show-count="true" aria-label="Star buttons/github-buttons on GitHub">Star</a> </li>
            <li>Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, <b>Jingfeng Yang</b>, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, Julian McAuley. 2024. <b>MEMORYLLM: Towards Self-Updatable Large Language Models.</b> ICML 2024. <a href="https://arxiv.org/abs/2402.04624" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Xin Liu, Zheng Li, Yifan Gao, <b>Jingfeng Yang</b>, Tianyu Cao, Zhengyang Wang, Bing Yin, Yangqiu Song. 2023. <b>Enhancing User Intent Capture in Session-Based Recommendation with Attribute Patterns.</b> NeurIPS 2023. <a href="https://arxiv.org/abs/2312.16199" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Caleb Ziems&#42;, William Held&#42;, <b>Jingfeng Yang</b>, Diyi Yang. 2022. <b>Multi-VALUE: A Framework for Cross-Dialectal English NLP.</b> ACL 2023. <a href="https://arxiv.org/pdf/2212.08011.pdf" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Xutan Peng, Yipeng Zhang, <b>Jingfeng Yang</b>, Mark Stevenson. 2022. <b>On the Security Vulnerabilities of Text-to-SQL Models.</b> ISSRE 2023 (<font color="red">1 of 3 Best Paper Candidates</font>). <a href="https://arxiv.org/pdf/2211.15363.pdf" style="color:#4133ff;">[Paper]</a>  </li>
            <li>Ruijie Wang, Zheng Li, <b>Jingfeng Yang</b>, Tianyu Cao, Chao Zhang, Bing Yin, Tarek Abdelzaher. 2023. <b>Mutually-paced Knowledge Distillation for Cross-lingual Temporal Knowledge Graph Reasoning.</b> In WWW' 2023. <a href="" style="color:#4133ff;">[Paper]</a></li>
            <li><b>Jingfeng Yang</b>&#42;, Le Zhang&#42;, Diyi Yang. 2022. <b>SUBS: Subtree Substitution for Compositional Semantic Parsing.</b> In NAACL' 2022. <a href="https://arxiv.org/pdf/2205.01538.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/GT-SALT/SUBS" style="color:#4133ff;">[Code]</a> <a href="resources/slides/SUBS_slides.pdf" style="color:#4133ff;">[Slides]</a> <a href="https://mp.weixin.qq.com/s/MKmh0BVYr1gjucjuYE7MoQ" style="color:#4133ff;">[Blog (in Chinese)]</a> </li>
            <li><b>Jingfeng Yang</b>, Federico Fancellu, Bonnie Webber, Diyi Yang. 2021. <b>Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing.</b> In EMNLP' 2021. <a href="https://aclanthology.org/2021.emnlp-main.472.pdf" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/JingfengYang/Multilingual-DRS-Semantic-parsing" style="color:#4133ff;">[Code]</a> <a href="resources/slides/xSP_slides.pdf" style="color:#4133ff;">[Slides]</a> </li>
            <li>Yang Zhong, <b>Jingfeng Yang</b>, Wei Xu, Diyi Yang. 2021. <b>WIKIBIAS: Detecting Multi-Span Subjective Biases in Language.</b> In EMNLP' 2021 (Findings). <a href="https://aclanthology.org/2021.findings-emnlp.155.pdf" style="color:#4133ff;">[Paper]</a> </li>
            <li><b>Jingfeng Yang</b>, Zhaoran Ma, Diyi Yang. 2020. <b>Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection.</b> In EMNLP' 2020. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.113/" style="color:#4133ff;">[Paper]</a> <a href="https://github.com/GT-SALT/Disfluency-Generation-and-Detection" style="color:#4133ff;">[Code]</a> <a href="resources/slides/disfluency_slides.pdf" style="color:#4133ff;">[Slides]</a> </li>
            <li><b>Jingfeng Yang</b>, Sujian Li. 2018. <b>Chinese Discourse Segmentation Using Bilingual Discourse Commonality.</b> Preprint. <a href="https://arxiv.org/abs/1809.01497" style="color:#4133ff;">[Paper]</a> </li>
	        <li>Yizhong Wang, Sujian Li, <b>Jingfeng Yang</b>. 2018. <b>Toward Fast and Accurate Neural Discourse Segmentation.</b> EMNLP' 2018. <a href="https://www.aclweb.org/anthology/D18-1116" style="color:#4133ff;">[Paper]</a></li>
	        <li>Yizhong Wang, Sujian Li, <b>Jingfeng Yang</b>, Xu Sun, Houfeng Wang. 2017. <b>Tag-enhanced Tree-structured Neural Networks for Implicit Discourse Relation Classification.</b> In The 8th International Joint Conference on Natural Language Processing (IJCNLP' 2017). <a href="https://arxiv.org/abs/1803.01165" style="color:#4133ff;">[Paper]</a></li>

        </ul>
        </div>

    <a name="service"></a>
    <div class="post-list">
            <h2 class="post-title">
            Service
            </h2> 
            <p>Reviewer/PC member in NLP Conferences: ACL Rolling Review, ACL' 2023, EMNLP' 2023, EMNLP' 2022, NAACL' 2021</p>
            <p>Reviewer/PC member in ML Conferences: NeurIPS' 2022, ICML' 2023</p>
            <p>Reviewer/PC member in AI Conferences: AAAI' 2023, IJCAI' 2023</p>
            <p>Reviewer/PC member in Data Mining Conferences/Transactions: KDD' 2023, TKDE' 2022</p>
    </div>
    <a name="research"></a>
    <div class="post-list">
        <h2 class="post-title">
        Research Experiences
    	</h2> 
	<p>Research Assistant in College of Computing, Georgia Institute of Technology. Advisor: <a href="https://cs.stanford.edu/~diyiy/" style="color:#4133ff;">Diyi Yang</a>. Aug 2019 - May 2021. </p>
    <p>Visiting Researcher in Institute for Language, Cognition and Computation, The University of Edinburgh. Advisor: <a href="https://www.aclweb.org/portal/content/bonnie-webber-receives-2020-acl-life-time-achievement-award" style="color:#4133ff;">Bonnie Webber</a>. July 2018 - Sep 2018. </p>
	<p>Research Assistant in Department of Computational Linguistics, Peking University. Advisor: <a href="https://pku-tangent.github.io/" style="color:#4133ff;">Sujian Li</a>. July 2017 - June 2019.</p>
    </div>
    <a name="work"></a>
    <div class="post-list">
        <h2 class="post-title">
        Industry Experiences
        </h2> 
        <p>Applied Scientist, Amazon, Palo Alto. Jan 2022 - present. </p>
        <p>Applied Scientist Intern, Amazon, Palo Alto (Virtual). Mentors: <a href="https://hmjianggatech.github.io/" style="color:#4133ff;">Haoming Jiang</a>, <a href="https://danqingz.github.io/" style="color:#4133ff;">Danqing Zhang</a>, <a href="https://scholar.google.com/citations?user=P-mBKNYAAAAJ&hl=zh-CN" style="color:#4133ff;">Qingyu Yin</a>, <a href="https://www.linkedin.com/in/bingyin/" style="color:#4133ff;">Bing Yin</a>. Sep 2021 - Dec 2021. </p>
        <p>Research Intern, Google, Mountain View (Virtual). Collaborators: <a href="https://research.google/people/AdityaGupta/" style="color:#4133ff;">Aditya Gupta</a>,  <a href="https://scholar.google.com/citations?user=i7lw4LwAAAAJ&hl=en" style="color:#4133ff;">Shyam Upadhyay</a>, <a href="https://scholar.google.com/citations?user=XVVvabUAAAAJ&hl=en" style="color:#4133ff;">Luheng He</a>, <a href="https://scholar.google.com/citations?user=e6uk9EAAAAAJ&hl=en" style="color:#4133ff;">Rahul Goel</a>, <a href="https://scholar.google.com/citations?user=a_95_VkAAAAJ&hl=en" style="color:#4133ff;">Shachi Paul</a>. July 2021 - Sep 2021.</p>
        <p>Software Development Engineer Intern, Amazon, San Francisco (Virtual). May 2020 - July 2020. </p>
        <p>Research Intern, Microsoft Research Asia, Beijing, China. Mentor: <a href="https://aclanthology.org/people/j/jin-ge-yao/" style="color:#4133ff;">Jin-ge Yao</a>. December 2018 - March 2019. </p>
    </div>
    <a name="talks"></a>
    <div class="post-list">
        <h2 class="post-title">
        Talks
        </h2> 
        <p><b>July, 2023:</b> Gave several talks on Artificial Superintelligence: Autonomous Agent and Super-alignment at Peking University and WAIC. <a href="resources/slides/ASI.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Sep, 2023:</b> Gave several talks on LLMs: Practices, Paradigm Shifts, Remaining Challenges at Meta and <a href="https://www.linkedin.com/feed/update/urn:li:activity:7105613549162397696/" style="color:#4133ff;">Amazon</a>. <a href="resources/slides/LLMs_Overall.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>April, 2023:</b> Gave several talks on Paradigm Shifts and Remaining Challenges Towards AGI at Google Brain and Michigan State University. <a href="resources/slides/Challenges_Towards_AGI.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>March, 2023:</b> Gave a talk on GPT series and NLP future directions at Peking University. <a href="resources/slides/GPT_Peking.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Feb, 2023:</b> Gave several talks on Reproduction and Usage of GPT-3/ChatGPT at Rice University, Texas A&M University, Intuit AI Research, <a href="https://event.baai.ac.cn/activities/654" style="color:#4133ff;">BAAI</a> etc. <a href="resources/slides/GPT_3.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Nov 17th, 2022:</b> Gave a talk on Table Understanding and Text2SQL in Alibaba DAMO Academy. <a href="resources/slides/Table_Understanding_and_Text2SQL.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Aug 4th, 2022:</b> Gave a talk on Compositional Generalization in LLM Era at <a href="https://event.baai.ac.cn/activities/489" style="color:#4133ff;">BAAI Seminar</a>. <a href="resources/slides/Compositional_Generalization.pdf" style="color:#4133ff;">[Slides]</a></p> 
        <p><b>Jun 10th, 2022:</b> Gave a talk on TableFormer in A9 ML Talk Series. <a href="resources/slides/Amazon_TableFormer.pdf" style="color:#4133ff;">[Slides]</a></p>
        <p><b>Mar 28th, 2021:</b> Gave some lectures for <a href="https://sites.google.com/view/nlp-bootcamp-s21/home" style="color:#4133ff;">NLP bootcamp</a> as a TA. </p>
        <p><b>Feb 15th, 2021:</b> Gave a lecture on PyTorch as head TA in <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/" style="color:#4133ff;">CS-4650 Natural Language Processing.</a> <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/slides/Introduction_to_PyTorch.pdf" style="color:#4133ff;">[Slides]</a></p>
        <p><b>Sep 2nd, 2020:</b> Gave a lecture on Deep Learning as a TA in <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing.</a> <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/slides/Introduction_to_NN.pdf" style="color:#4133ff;">[Slides]</a></p>
    </div>
    <a name="teaching"></a>
    <div class="post-list">
        <h2 class="post-title">
        Teaching Experiences
        </h2> 
        <p>Teaching Assitant for Coding Support, POLS-585 Text as Data, Emory University, Atlanta. June 2021 - July 2021. </p>
        <p>Head Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2021/cs4650_spring/" style="color:#4133ff;">CS-4650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Spring 2021. </p>
        <p>Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2021/cs7650_fall/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Fall 2020. </p>
	    <p>Teaching Assistant, <a href="https://www.cc.gatech.edu/classes/AY2020/cs7650_spring/" style="color:#4133ff;">CS-4650/7650 Natural Language Processing</a>, Georgia Institue of Technology, Atlanta. Spring 2020. </p>
    </div>
    
    <a name="awards"></a>
    <div class="post-list">
        <h2 class="post-title">
        Awards 
    	</h2> 
        <p>May 4th Fellowship, 2016-2017.</p>
	    <p>Kwang-Hua Fellowship, 2015-2016.</p>
	    <p>Merit Student of Peking University, 2016-2017, 2015-2016.</p>
	    <p>Silver medalist in Chinese Mathematics Olympiad (CMO), 2015.</p>
	
    </div>

    <a name="intern"></a>
    <div class="post-list">
        <h2 class="post-title">
        Intern Mentorship 
    	</h2> 
        <p><a href="https://night-chen.github.io/" style="color:#4133ff;">Yuchen Zhuang</a>, Georgia Tech. Summer 2024.</p>
        <p><a href="https://zilongwang.me/" style="color:#4133ff;">Zilong Wang</a>, UCSD. Summer 2024.</p>
        <p><a href="https://mooler0410.github.io/puguJin/" style="color:#4133ff;">Hongye Jin</a>, Texas A&M University. Summer 2024.</p>
        <p><a href="https://vivian1993.github.io/" style="color:#4133ff;">Kewei Cheng</a>, UCLA. Fall 2023.</p>
        <p><a href="https://ahxt.github.io/" style="color:#4133ff;">Xiaotian Han</a>, Texas A&M University. Summer 2023.</p>
        <p><a href="https://yuwang.org/" style="color:#4133ff;">Yu Wang</a>, UCSD. Summer 2023.</p>
        <p><a href="https://www.cse.msu.edu/~wenhongz/" style="color:#4133ff;">Hongzhi Wen</a>, Michigan State University. Summer 2023.</p>
        <p><a href="http://www.zhimengjiang.com/" style="color:#4133ff;">Zhimeng Jiang</a>, Texas A&M University. Fall 2022.</p>
        <p><a href="https://www.cs.toronto.edu/~zining/" style="color:#4133ff;">Zining Zhu</a>, University of Toronto. Summer 2022.</p>
        <p><a href="https://wjerry5.github.io/" style="color:#4133ff;">Ruijie Wang</a>, UIUC. Summer 2022.</p>
        <p><a href="https://jeffhj.github.io/" style="color:#4133ff;">Jie Huang</a>, UIUC. Summer 2022.</p>
	    <p><a href="https://seanliu96.github.io/" style="color:#4133ff;">Xin Liu</a>, HKUST. Summer 2022.</p>
	
    </div>



    
</div>
